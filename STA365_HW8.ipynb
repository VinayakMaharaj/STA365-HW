{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Describe how the posterior predictive distribution is created for mixture models"
      ],
      "metadata": {
        "id": "1kenrR3g6AjV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In mixture models, the posterior predictive distribution is calculated as follows:\n",
        "\n",
        "1. Parameter Estimation: Estimate the parameters of the mixture model, such as means, variances, and mixing coefficients.\n",
        "\n",
        "2. Posterior Distribution: Compute the posterior distribution of these parameters given the observed data, applying Bayes' theorem:\n",
        "\n",
        "$$\n",
        "p(\\theta | X) = \\frac{p(X | \\theta) p(\\theta)}{p(X)}\n",
        "$$\n",
        "\n",
        "where \\( \\theta \\) represents the parameters of the mixture model, \\( X \\) is the observed data, \\( p(X | \\theta) \\) is the likelihood of the data given the parameters, \\( p(\\theta) \\) is the prior, and \\( p(X) \\) is the evidence or marginal likelihood of the data.\n",
        "\n",
        "3. Predictive Distribution for New Data: Integrate over all possible values of the parameters to get the predictive distribution for a new data point \\( \\tilde{x} \\):\n",
        "\n",
        "$$\n",
        "p(\\tilde{x} | X) = \\int p(\\tilde{x} | \\theta) p(\\theta | X) d\\theta\n",
        "$$\n",
        "\n",
        "4. Account for Mixture Components: For each mixture component, calculate the likelihood of the new data point and take a weighted average according to the mixing coefficients. This step involves summing over the component-specific predictive distributions, weighted by their respective posterior probabilities:\n",
        "\n",
        "$$\n",
        "p(\\tilde{x} | X) = \\sum_{k=1}^{K} \\pi_k p(\\tilde{x} | \\theta_k)\n",
        "$$\n",
        "\n",
        "where \\( K \\) is the number of components in the mixture model, \\( \\pi_k \\) are the mixing coefficients, and \\( \\theta_k \\) are the component-specific parameters.\n",
        "\n",
        "5. Sampling Methods: If the integral in step 3 cannot be computed analytically, use sampling methods such as Markov Chain Monte Carlo (MCMC) to approximate the posterior predictive distribution. These methods generate samples from the posterior distribution of the parameters, which can then be used to approximate the predictive distribution for new data points.\n"
      ],
      "metadata": {
        "id": "vQS-dqtI6IOz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Describe how the posterior predictive distribution is created in general"
      ],
      "metadata": {
        "id": "6cYVY8FI7DXG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The posterior predictive distribution is created in the following general steps:\n",
        "\n",
        "1. Collect Data: Obtain the data set \\( X \\).\n",
        "\n",
        "2. Specify Prior Distribution: Choose a prior \\( p(\\theta) \\) for the parameters, reflecting our prior beliefs about the parameters before observing the data.\n",
        "\n",
        "3. Specify Likelihood Function: Define \\( p(X | \\theta) \\), which models how likely the observed data is for different values of the parameters.\n",
        "\n",
        "4. Compute Posterior Distribution: Apply Bayes' theorem to update our belief about the parameters based on the observed data, resulting in \\( p(\\theta | X) \\):\n",
        "\n",
        "$$\n",
        "p(\\theta | X) = \\frac{p(X | \\theta) p(\\theta)}{p(X)}\n",
        "$$\n",
        "\n",
        "where \\( p(X) \\) is a normalizing constant, also known as the marginal likelihood or evidence, which ensures that the posterior distribution sums (or integrates) to 1.\n",
        "\n",
        "5. Define Predictive Distribution: For a new observation \\( \\tilde{x} \\), compute the predictive distribution, which is the distribution of \\( \\tilde{x} \\) given the observed data \\( X \\), by integrating over all possible parameter values, weighted by their posterior probability:\n",
        "\n",
        "$$\n",
        "p(\\tilde{x} | X) = \\int p(\\tilde{x} | \\theta) p(\\theta | X) d\\theta\n",
        "$$\n",
        "\n",
        "This step captures the essence of Bayesian prediction, incorporating both the uncertainty about the parameters and the model's predictive capabilities.\n",
        "\n",
        "6. Approximation Methods: When the integral in step 5 is intractable, which is common in complex models, use approximation methods such as Markov Chain Monte Carlo (MCMC), Variational Inference, or Laplace Approximation to estimate the predictive distribution. These methods allow us to approximate the integral by sampling parameter values from the posterior distribution or by finding a simpler distribution that closely approximates the posterior.\n",
        "\n",
        "7. Evaluate Predictive Performance: Optionally, evaluate the predictive performance of the model using techniques such as cross-validation or predictive checks. This step involves comparing the predictive distribution to actual observed outcomes to assess how well the model predicts new data.\n",
        "\n",
        "These steps outline the process of generating a posterior predictive distribution in a Bayesian framework, highlighting the central role of Bayes' theorem and the importance of incorporating both prior information and observed data in making predictions.\n",
        "\n"
      ],
      "metadata": {
        "id": "8HhthP8w6-Gk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Question 3"
      ],
      "metadata": {
        "id": "xb0fy2CX7jzZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "When conducting a Bayesian regression of \\( y \\) on \\( X \\) where \\( X \\) contains missing values, the missing data can be addressed within the Bayesian framework without the need to discard observations. This approach involves treating the missing data as latent variables.\n",
        "\n",
        "Firstly, the missing data in \\( X \\) are treated as latent variables with a prior distribution that reflects our beliefs about the mechanism of missingness. Next, a joint probabilistic model for both the observed data and the latent variables is constructed to capture the relationships and dependencies.\n",
        "\n",
        "Bayesian inference is then used to compute the posterior distribution of the missing values, based on the observed data and the latent variables. This process involves iterative sampling from the posterior distribution, using techniques such as Markov Chain Monte Carlo (MCMC), which alternates between imputing the missing values and updating the model parameters.\n",
        "\n",
        "It is important to assess the Missing Completely at Random (MCAR) assumption during this process. If the MCAR assumption is violated, the model should be adjusted to account for Missing at Random (MAR) or Not Missing at Random (MNAR) conditions.\n",
        "\n",
        "This methodology allows for the use of all available data, providing a more comprehensive analysis and potentially leading to more accurate inferences than simply discarding incomplete cases.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "F-ddxJc-7nY5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pymc as pm\n",
        "import numpy as np\n",
        "\n",
        "# Assume y is our dependent variable and X is our predictor with some missing values.\n",
        "np.random.seed(123)\n",
        "n_samples = 100\n",
        "X = np.random.normal(size=n_samples)\n",
        "y = 2 * X + np.random.normal(size=n_samples)\n",
        "\n",
        "# Introduce missing data in X\n",
        "missing_rate = 0.2\n",
        "missing_indices = np.random.choice(np.arange(n_samples), replace=False, size=int(n_samples * missing_rate))\n",
        "X[missing_indices] = np.nan\n",
        "\n",
        "with pm.Model() as model:\n",
        "    # Impute missing values in X\n",
        "    X_imputed = pm.Normal('X_imputed', mu=0, sigma=10, observed=X)\n",
        "\n",
        "    # Prior distributions for the regression coefficients\n",
        "    alpha = pm.Normal('alpha', mu=0, sigma=10)\n",
        "    beta = pm.Normal('beta', mu=0, sigma=10)\n",
        "    sigma = pm.HalfNormal('sigma', sigma=1)\n",
        "\n",
        "    # Expected value of outcome, using imputed X\n",
        "    mu = alpha + beta * X_imputed\n",
        "\n",
        "    # Likelihood (sampling distribution) of observations\n",
        "    Y_obs = pm.Normal('Y_obs', mu=mu, sigma=sigma, observed=y)\n",
        "\n",
        "    # Sample from the posterior using the NUTS sampler\n",
        "    trace = pm.sample(1000, return_inferencedata=False)\n",
        "\n",
        "# After the model has run, you can analyze the trace to see the imputed values for X\n",
        "# and the inferred parameters (alpha, beta, sigma).\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 112
        },
        "id": "nPPr1uv2W6KD",
        "outputId": "bd2be4df-4650-4247-99f3-206552a8b54d"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/pymc/model/core.py:1316: ImputationWarning: Data in X_imputed contains missing values and will be automatically imputed from the sampling distribution.\n",
            "  warnings.warn(impute_message, ImputationWarning)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "<style>\n",
              "    /* Turns off some styling */\n",
              "    progress {\n",
              "        /* gets rid of default border in Firefox and Opera. */\n",
              "        border: none;\n",
              "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
              "        background-size: auto;\n",
              "    }\n",
              "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
              "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
              "    }\n",
              "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
              "        background: #F44336;\n",
              "    }\n",
              "</style>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      <progress value='2000' class='' max='2000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      100.00% [2000/2000 00:05&lt;00:00 Sampling chain 0, 0 divergences]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "<style>\n",
              "    /* Turns off some styling */\n",
              "    progress {\n",
              "        /* gets rid of default border in Firefox and Opera. */\n",
              "        border: none;\n",
              "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
              "        background-size: auto;\n",
              "    }\n",
              "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
              "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
              "    }\n",
              "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
              "        background: #F44336;\n",
              "    }\n",
              "</style>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      <progress value='2000' class='' max='2000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      100.00% [2000/2000 00:05&lt;00:00 Sampling chain 1, 0 divergences]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code directly uses pm.Normal with the observed=X argument, where X contains nan values for missing entries. PyMC will automatically treat these nan values as missing data to be imputed during the model fitting process."
      ],
      "metadata": {
        "id": "nGeS_ZxPXOIU"
      }
    }
  ]
}